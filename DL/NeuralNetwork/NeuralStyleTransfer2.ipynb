{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989cd00f",
   "metadata": {},
   "source": [
    "# Neural Style Transfer (VGG19, PyTorch) â€” **Tuned to Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31aaa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, numpy as np\n",
    "import torch, torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(\"Using device:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"CUDA:\", torch.cuda.get_device_name(0))\n",
    "elif DEVICE.type == \"mps\":\n",
    "    print(\"Apple Metal (MPS) backend active\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257803b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTENT_PATH = None\n",
    "STYLE_PATH   = None\n",
    "\n",
    "def make_placeholder(size=512):\n",
    "    w=h=size\n",
    "    x = np.linspace(0,1,w); y = np.linspace(0,1,h)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    r = (np.sin(2*np.pi*X*3)+1)/2\n",
    "    g = (np.cos(2*np.pi*Y*3)+1)/2\n",
    "    b = (np.sin(2*np.pi*(X+Y)*2)+1)/2\n",
    "    img_np = np.stack([r,g,b], axis=2)\n",
    "    img_np[40:110, 40:160, :] = [1.0, 0.6, 0.2]\n",
    "    cy, cx, rad = 180, 190, 35\n",
    "    yy, xx = np.ogrid[:h, :w]\n",
    "    mask = (yy-cy)**2 + (xx-cx)**2 <= rad**2\n",
    "    img_np[mask] = [0.2, 0.7, 1.0]\n",
    "    return Image.fromarray((img_np*255).astype(np.uint8))\n",
    "\n",
    "def load_pil(path, fallback_size=512):\n",
    "    if path and os.path.exists(path):\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "    return make_placeholder(fallback_size)\n",
    "\n",
    "content_disp = load_pil(CONTENT_PATH, 512)\n",
    "style_disp   = load_pil(STYLE_PATH,   512)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "ax[0].imshow(content_disp); ax[0].set_title(\"Content\"); ax[0].axis(\"off\")\n",
    "ax[1].imshow(style_disp);   ax[1].set_title(\"Style\");   ax[1].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    weights = models.VGG19_Weights.DEFAULT\n",
    "    vgg_features = models.vgg19(weights=weights).features.eval().to(DEVICE)\n",
    "    imagenet_mean = torch.tensor(weights.meta[\"mean\"]).view(1,3,1,1).to(DEVICE)\n",
    "    imagenet_std  = torch.tensor(weights.meta[\"std\"]).view(1,3,1,1).to(DEVICE)\n",
    "    print(\"Loaded VGG19 pretrained on ImageNet.\")\n",
    "except Exception as e:\n",
    "    print(\"WARNING: Using random init; results will be poor.\\n\", e)\n",
    "    vgg_features = models.vgg19(weights=None).features.eval().to(DEVICE)\n",
    "    imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(DEVICE)\n",
    "    imagenet_std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(DEVICE)\n",
    "\n",
    "def make_preprocess(imsize):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(imsize, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean.squeeze().tolist(), std=imagenet_std.squeeze().tolist()),\n",
    "    ])\n",
    "\n",
    "def pil_to_tensor(img, imsize):\n",
    "    pre = make_preprocess(imsize)\n",
    "    return pre(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def tensor_to_pil(t):\n",
    "    x = t.detach().cpu().clone()\n",
    "    x = x * imagenet_std.cpu() + imagenet_mean.cpu()\n",
    "    x = x.clamp(0,1)\n",
    "    return transforms.ToPILImage()(x.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTENT_LAYERS = [10, 21]\n",
    "STYLE_LAYERS   = [0, 5, 10, 19, 28]\n",
    "STYLE_WEIGHTS_PER_LAYER = [1, 1, 1, 1, 1]\n",
    "\n",
    "def forward_features(x, layers_to_capture):\n",
    "    feats = {}\n",
    "    out = x\n",
    "    for i, layer in enumerate(vgg_features):\n",
    "        out = layer(out)\n",
    "        if i in layers_to_capture:\n",
    "            feats[i] = out\n",
    "        if len(feats) == len(layers_to_capture) and i >= max(layers_to_capture):\n",
    "            break\n",
    "    return feats\n",
    "\n",
    "def gram_matrix(x):\n",
    "    B, C, H, W = x.shape\n",
    "    y = x.view(B, C, H*W)\n",
    "    G = y @ y.transpose(1, 2)\n",
    "    return G / (C*H*W)\n",
    "\n",
    "def total_variation(x):\n",
    "    return (x[:, :, :-1, :] - x[:, :, 1:, :]).abs().mean() + (x[:, :, :, :-1] - x[:, :, :, 1:]).abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_style_transfer(content_img, style_img, imsize=512, steps=400,\n",
    "                       content_weight=5.0, style_weight=2e3, tv_weight=1.5e-3,\n",
    "                       init_from=\"content\", preview_every=50, out_path=\"stylized.png\"):\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tgt_c = forward_features(content_img, CONTENT_LAYERS)\n",
    "        tgt_s = forward_features(style_img,   STYLE_LAYERS)\n",
    "        tgt_s_grams = {i: gram_matrix(tgt_s[i]) for i in STYLE_LAYERS}\n",
    "\n",
    "    if init_from == \"content\":\n",
    "        generated = content_img.clone().detach()\n",
    "    elif init_from == \"style\":\n",
    "        generated = style_img.clone().detach()\n",
    "    else:\n",
    "        generated = torch.randn_like(content_img) * 0.1 + content_img * 0.9\n",
    "\n",
    "    generated.requires_grad_(True)\n",
    "    opt = torch.optim.LBFGS([generated], max_iter=steps, lr=1.0)\n",
    "\n",
    "    counter = [0]\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        feats_c = forward_features(generated, CONTENT_LAYERS)\n",
    "        feats_s = forward_features(generated, STYLE_LAYERS)\n",
    "        c_loss = sum(F.mse_loss(feats_c[i], tgt_c[i]) for i in CONTENT_LAYERS)\n",
    "        s_loss = sum(\n",
    "            w * F.mse_loss(gram_matrix(feats_s[i]), tgt_s_grams[i])\n",
    "            for w, i in zip(STYLE_WEIGHTS_PER_LAYER, STYLE_LAYERS)\n",
    "        )\n",
    "        tv = total_variation(generated)\n",
    "        loss = content_weight*c_loss + style_weight*s_loss + tv_weight*tv\n",
    "        loss.backward()\n",
    "\n",
    "        counter[0] += 1\n",
    "        if counter[0] % preview_every == 0 or counter[0] in (1, steps):\n",
    "            if DEVICE.type == \"mps\":\n",
    "                torch.mps.synchronize()\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Step {counter[0]}/{steps} | content {c_loss.item():.4f} | style {s_loss.item():.4f} | tv {tv.item():.4f}\")\n",
    "            display(tensor_to_pil(generated))\n",
    "        return loss\n",
    "\n",
    "    t0 = time.time()\n",
    "    opt.step(closure)\n",
    "    out = tensor_to_pil(generated); out.save(out_path)\n",
    "    torch.set_grad_enabled(False)\n",
    "    print(f\"Saved: {out_path}  |  Elapsed: {time.time()-t0:.1f}s\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034e6987",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two-stage run for clearer results\n",
    "IMSIZE1 = 256\n",
    "content_t1 = pil_to_tensor(content_disp, IMSIZE1)\n",
    "style_t1   = pil_to_tensor(style_disp,   IMSIZE1)\n",
    "tmp = run_style_transfer(content_t1, style_t1, imsize=IMSIZE1, steps=200,\n",
    "                         content_weight=3.0, style_weight=2e3, tv_weight=1e-3,\n",
    "                         init_from=\"content\", preview_every=50, out_path=\"stylized_256.png\")\n",
    "\n",
    "IMSIZE2 = 512\n",
    "init512 = pil_to_tensor(tmp, IMSIZE2)\n",
    "style_t2 = pil_to_tensor(style_disp, IMSIZE2)\n",
    "final_out = run_style_transfer(init512, style_t2, imsize=IMSIZE2, steps=350,\n",
    "                               content_weight=5.0, style_weight=2e3, tv_weight=1.5e-3,\n",
    "                               init_from=\"content\", preview_every=50, out_path=\"stylized_512.png\")\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "ax[0].imshow(content_disp.resize((IMSIZE2,IMSIZE2))); ax[0].set_title(\"Content\"); ax[0].axis(\"off\")\n",
    "ax[1].imshow(style_disp.resize((IMSIZE2,IMSIZE2)));   ax[1].set_title(\"Style\");   ax[1].axis(\"off\")\n",
    "ax[2].imshow(final_out);                               ax[2].set_title(\"Stylized\"); ax[2].axis(\"off\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
