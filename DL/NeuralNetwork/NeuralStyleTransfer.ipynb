{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70289d3",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŽ¨ Neural Style Transfer with VGG19 â€” *Interactive Notebook* (PyTorch, CUDA/MPS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a277ee3",
   "metadata": {},
   "source": [
    "\n",
    "> Optional installation (uncomment & run if needed):\n",
    "```bash\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b41089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, math, time, numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "except Exception:\n",
    "    widgets = None\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(\"Using device:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    print(\"CUDA:\", torch.cuda.get_device_name(0))\n",
    "elif DEVICE.type == \"mps\":\n",
    "    print(\"Apple Metal (MPS) backend active\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTENT_PATH = None\n",
    "STYLE_PATH   = None\n",
    "\n",
    "def make_placeholder(size=512):\n",
    "    w=h=size\n",
    "    x = np.linspace(0,1,w); y = np.linspace(0,1,h)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    r = (np.sin(2*np.pi*X*3)+1)/2\n",
    "    g = (np.cos(2*np.pi*Y*3)+1)/2\n",
    "    b = (np.sin(2*np.pi*(X+Y)*2)+1)/2\n",
    "    img_np = np.stack([r,g,b], axis=2)\n",
    "    img_np[40:110, 40:160, :] = [1.0, 0.6, 0.2]\n",
    "    cy, cx, rad = 180, 190, 35\n",
    "    yy, xx = np.ogrid[:h, :w]\n",
    "    mask = (yy-cy)**2 + (xx-cx)**2 <= rad**2\n",
    "    img_np[mask] = [0.2, 0.7, 1.0]\n",
    "    return Image.fromarray((img_np*255).astype(np.uint8))\n",
    "\n",
    "def load_pil(path, fallback_size=512):\n",
    "    if path and os.path.exists(path):\n",
    "        return Image.open(path).convert(\"RGB\")\n",
    "    return make_placeholder(fallback_size)\n",
    "\n",
    "content_disp = load_pil(CONTENT_PATH, 512)\n",
    "style_disp   = load_pil(STYLE_PATH,   512)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "ax[0].imshow(content_disp); ax[0].set_title(\"Content\"); ax[0].axis(\"off\")\n",
    "ax[1].imshow(style_disp);   ax[1].set_title(\"Style\");   ax[1].axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08657bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    weights = models.VGG19_Weights.DEFAULT\n",
    "    vgg_features = models.vgg19(weights=weights).features.eval().to(DEVICE)\n",
    "    imagenet_mean = torch.tensor(weights.meta[\"mean\"]).view(1,3,1,1).to(DEVICE)\n",
    "    imagenet_std  = torch.tensor(weights.meta[\"std\"]).view(1,3,1,1).to(DEVICE)\n",
    "    print(\"Loaded VGG19 pretrained on ImageNet.\")\n",
    "except Exception as e:\n",
    "    print(\"WARNING: Using random init; results will be poor.\\n\", e)\n",
    "    vgg_features = models.vgg19(weights=None).features.eval().to(DEVICE)\n",
    "    imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(DEVICE)\n",
    "    imagenet_std  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(DEVICE)\n",
    "\n",
    "def make_preprocess(imsize):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(imsize, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(imsize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean.squeeze().tolist(), std=imagenet_std.squeeze().tolist()),\n",
    "    ])\n",
    "\n",
    "def pil_to_tensor(img, imsize):\n",
    "    pre = make_preprocess(imsize)\n",
    "    return pre(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "def tensor_to_pil(t):\n",
    "    x = t.detach().cpu().clone()\n",
    "    x = x * imagenet_std.cpu() + imagenet_mean.cpu()\n",
    "    x = x.clamp(0,1)\n",
    "    return transforms.ToPILImage()(x.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c81b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTENT_LAYERS = [21]                         # conv4_2\n",
    "STYLE_LAYERS   = [0, 5, 10, 19, 28]           # conv1_1..conv5_1\n",
    "STYLE_WEIGHTS_PER_LAYER = [1, 1, 1, 1, 1]\n",
    "\n",
    "def forward_features(x, layers_to_capture):\n",
    "    feats = {}\n",
    "    out = x\n",
    "    for i, layer in enumerate(vgg_features):\n",
    "        out = layer(out)\n",
    "        if i in layers_to_capture:\n",
    "            feats[i] = out\n",
    "        if len(feats) == len(layers_to_capture) and i >= max(layers_to_capture):\n",
    "            break\n",
    "    return feats\n",
    "\n",
    "def gram_matrix(x):\n",
    "    B, C, H, W = x.shape\n",
    "    y = x.view(B, C, H*W)\n",
    "    G = y @ y.transpose(1, 2)\n",
    "    return G / (C*H*W)\n",
    "\n",
    "def total_variation(x):\n",
    "    return (x[:, :, :-1, :] - x[:, :, 1:, :]).abs().mean() + (x[:, :, :, :-1] - x[:, :, :, 1:]).abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8eb435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_style_transfer(content_img, style_img, imsize=512, steps=400, lr=0.03,\n",
    "                       content_weight=1.0, style_weight=1e3, tv_weight=1e-3,\n",
    "                       init_from=\"content\", preview_every=50, out_path=\"stylized.png\"):\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tgt_c = forward_features(content_img, CONTENT_LAYERS)\n",
    "        tgt_s = forward_features(style_img,   STYLE_LAYERS)\n",
    "        tgt_s_grams = {i: gram_matrix(tgt_s[i]) for i in STYLE_LAYERS}\n",
    "\n",
    "    if init_from == \"content\":\n",
    "        generated = content_img.clone().detach()\n",
    "    elif init_from == \"style\":\n",
    "        generated = style_img.clone().detach()\n",
    "    else:\n",
    "        generated = torch.randn_like(content_img) * 0.1 + content_img * 0.9\n",
    "\n",
    "    generated.requires_grad_(True)\n",
    "    opt = torch.optim.Adam([generated], lr=lr)\n",
    "\n",
    "    for step in range(1, steps+1):\n",
    "        opt.zero_grad()\n",
    "        feats_c = forward_features(generated, CONTENT_LAYERS)\n",
    "        feats_s = forward_features(generated, STYLE_LAYERS)\n",
    "\n",
    "        c_loss = sum(torch.nn.functional.mse_loss(feats_c[i], tgt_c[i]) for i in CONTENT_LAYERS)\n",
    "        s_loss = 0.0\n",
    "        for w, i in zip(STYLE_WEIGHTS_PER_LAYER, STYLE_LAYERS):\n",
    "            s_loss = s_loss + w * torch.nn.functional.mse_loss(gram_matrix(feats_s[i]), tgt_s_grams[i])\n",
    "        tv = total_variation(generated)\n",
    "\n",
    "        loss = content_weight*c_loss + style_weight*s_loss + tv_weight*tv\n",
    "        loss.backward(); opt.step()\n",
    "\n",
    "        if DEVICE.type == \"mps\":\n",
    "            torch.mps.synchronize()\n",
    "\n",
    "        if step % preview_every == 0 or step in (1, steps):\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Step {step}/{steps} | content {c_loss.item():.4f} | style {s_loss.item():.4f} | tv {tv.item():.4f}\")\n",
    "            display(tensor_to_pil(generated))\n",
    "\n",
    "    out_pil = tensor_to_pil(generated); out_pil.save(out_path)\n",
    "    torch.set_grad_enabled(False)\n",
    "    print(\"Saved:\", out_path)\n",
    "    return out_pil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initial tensors\n",
    "IMSIZE = 512\n",
    "content_disp = content_disp\n",
    "style_disp   = style_disp\n",
    "content_t = pil_to_tensor(content_disp, IMSIZE)\n",
    "style_t   = pil_to_tensor(style_disp,   IMSIZE)\n",
    "\n",
    "# Triptych\n",
    "fig, ax = plt.subplots(1,3, figsize=(12,4))\n",
    "ax[0].imshow(content_disp.resize((IMSIZE,IMSIZE))); ax[0].set_title(\"Content\"); ax[0].axis(\"off\")\n",
    "ax[1].imshow(style_disp.resize((IMSIZE,IMSIZE)));   ax[1].set_title(\"Style\");   ax[1].axis(\"off\")\n",
    "ax[2].imshow(content_disp.resize((IMSIZE,IMSIZE))); ax[2].set_title(\"Init (content)\"); ax[2].axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "if widgets is not None:\n",
    "    content_w = widgets.FloatSlider(value=1.0, min=0.1, max=20.0, step=0.1, description=\"Content\")\n",
    "    style_w   = widgets.FloatLogSlider(value=1e3, base=10, min=2, max=5, step=0.1, description=\"Style\")\n",
    "    tv_w      = widgets.FloatLogSlider(value=1e-3, base=10, min=-5, max=-1, step=0.1, description=\"TV\")\n",
    "    imsize_w  = widgets.IntSlider(value=IMSIZE, min=256, max=768, step=64, description=\"Size\")\n",
    "    steps_w   = widgets.IntSlider(value=400, min=100, max=1000, step=50, description=\"Steps\")\n",
    "    lr_w      = widgets.FloatSlider(value=0.03, min=0.005, max=0.1, step=0.005, description=\"LR\")\n",
    "    init_w    = widgets.Dropdown(options=[\"content\",\"style\",\"noise\"], value=\"content\", description=\"Init\")\n",
    "    out_path  = widgets.Text(value=\"stylized_output.png\", description=\"Save as\")\n",
    "    run_b     = widgets.Button(description=\"Run style transfer\", button_style=\"primary\")\n",
    "\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HBox([content_w, style_w, tv_w]),\n",
    "        widgets.HBox([imsize_w, steps_w, lr_w, init_w]),\n",
    "        out_path,\n",
    "        run_b\n",
    "    ])\n",
    "    display(ui)\n",
    "\n",
    "    def _on_click(_):\n",
    "        c_t = pil_to_tensor(content_disp, imsize_w.value)\n",
    "        s_t = pil_to_tensor(style_disp,   imsize_w.value)\n",
    "        run_style_transfer(\n",
    "            c_t, s_t,\n",
    "            imsize=imsize_w.value,\n",
    "            steps=steps_w.value,\n",
    "            lr=lr_w.value,\n",
    "            content_weight=content_w.value,\n",
    "            style_weight=style_w.value,\n",
    "            tv_weight=tv_w.value,\n",
    "            init_from=init_w.value,\n",
    "            preview_every=50,\n",
    "            out_path=out_path.value\n",
    "        )\n",
    "\n",
    "    run_b.on_click(_on_click)\n",
    "else:\n",
    "    print(\"ipywidgets not available; call run_style_transfer(content_t, style_t, ...) manually.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
