{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fd295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# ðŸ§© STEP 1 â€” Imports\n",
    "# ==============================\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary  # For model visualization\n",
    "from torchviz import make_dot       # Optional: graphical architecture view\n",
    "\n",
    "# ==============================\n",
    "# ðŸ§© STEP 2 â€” Dataset Loader\n",
    "# ==============================\n",
    "# Folder should contain subfolders of images, or all .jpg/.png images\n",
    "data_dir = \"/path/to/your/image/folder\"  # Change this!\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# ==============================\n",
    "# ðŸ§© STEP 3 â€” U-Net Definition\n",
    "# ==============================\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2d â†’ ReLU) Ã— 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(3, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(128, 256)\n",
    "\n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv1 = DoubleConv(256, 128)  # concat skip connection from down2\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv2 = DoubleConv(128, 64)   # concat skip connection from down1\n",
    "\n",
    "        # Output\n",
    "        self.output = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool1(d1))\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool2(d2))\n",
    "        # Decoder with skip connections\n",
    "        u1 = self.conv1(torch.cat([self.up1(b), d2], dim=1))\n",
    "        u2 = self.conv2(torch.cat([self.up2(u1), d1], dim=1))\n",
    "        # Output\n",
    "        return self.output(u2)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ðŸ§© STEP 4 â€” Model Visualization\n",
    "# ==============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNet().to(device)\n",
    "\n",
    "# Print model summary\n",
    "summary(model, (3, 256, 256))\n",
    "\n",
    "# Optional: visualize graphically (torchviz)\n",
    "x = torch.randn(1, 3, 256, 256).to(device)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(list(model.named_parameters()))).render(\"unet_graph\", format=\"png\")\n",
    "\n",
    "# ==============================\n",
    "# ðŸ§© STEP 5 â€” Training (Denoising Autoencoder)\n",
    "# ==============================\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def add_noise(img, noise_factor=0.2):\n",
    "    noise = torch.randn_like(img) * noise_factor\n",
    "    return torch.clamp(img + noise, 0., 1.)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        noisy_imgs = add_noise(imgs)\n",
    "        preds = model(noisy_imgs)\n",
    "        loss = criterion(preds, imgs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ==============================\n",
    "# ðŸ§© STEP 6 â€” Visualize Results\n",
    "# ==============================\n",
    "imgs, _ = next(iter(dataloader))\n",
    "noisy_imgs = add_noise(imgs)\n",
    "with torch.no_grad():\n",
    "    outputs = model(noisy_imgs.to(device)).cpu()\n",
    "\n",
    "def show_images(img_list, titles):\n",
    "    fig, axes = plt.subplots(1, len(img_list), figsize=(15,5))\n",
    "    for i, (img, title) in enumerate(zip(img_list, titles)):\n",
    "        axes[i].imshow(torch.permute(img[0], (1,2,0)))\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_images([imgs, noisy_imgs, outputs], [\"Original\", \"Noisy\", \"Denoised\"])\n",
    "\n",
    "# ==============================\n",
    "# ðŸ§© STEP 7 â€” Visualize Filters\n",
    "# ==============================\n",
    "filters = model.down1.block[0].weight.data.clone().cpu()\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(8):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    f = filters[i]\n",
    "    f = (f - f.min()) / (f.max() - f.min())\n",
    "    plt.imshow(f.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Example Filters from First Conv Layer\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
