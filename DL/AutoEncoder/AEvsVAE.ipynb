{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ae9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTOENCODER vs VARIATIONAL AUTOENCODER COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# ðŸ”§ CONFIGURATION\n",
    "# ============================================================\n",
    "import os, random, torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"axes.unicode_minus\": False,\n",
    "    \"text.latex.preamble\": r\"\\usepackage{amsmath}\",  # <-- add this\n",
    "})\n",
    "config = {\n",
    "    \"image_folder\": \"../../../SparseSamplingCS-AE-RL/data/faces/ai/\",\n",
    "    \"image_size\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 10,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"latent_dim\": 128,\n",
    "    \"train_split\": 0.7,\n",
    "    \"val_split\": 0.15,\n",
    "    \"num_workers\": 2,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# DEVICE SETUP\n",
    "# ============================================================\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(config[\"seed\"])\n",
    "np.random.seed(config[\"seed\"])\n",
    "random.seed(config[\"seed\"])\n",
    "\n",
    "# ============================================================\n",
    "# DATA\n",
    "# ============================================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((config[\"image_size\"], config[\"image_size\"])),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(config[\"image_folder\"], transform=transform)\n",
    "n_total = len(dataset)\n",
    "n_train = int(config[\"train_split\"] * n_total)\n",
    "n_val = int(config[\"val_split\"] * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "train_data, val_data, test_data = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"])\n",
    "val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n",
    "test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n",
    "\n",
    "print(f\"Train: {n_train}, Val: {n_val}, Test: {n_test}\")\n",
    "\n",
    "# ============================================================\n",
    "# BASE CONVOLUTIONAL ENCODER / DECODER\n",
    "# ============================================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# ============================================================\n",
    "# MODELS: AE and VAE\n",
    "# ============================================================\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        enc_out_dim = 128 * (config[\"image_size\"] // 8) ** 2\n",
    "        self.fc_mu = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(enc_out_dim, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, enc_out_dim)\n",
    "        self.decoder = Decoder()\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x).flatten(start_dim=1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def decode(self, z):\n",
    "        h = self.fc_dec(z)\n",
    "        h = h.view(-1, 128, config[\"image_size\"] // 8, config[\"image_size\"] // 8)\n",
    "        return self.decoder(h)\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "# ============================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return (recon_loss + kld) / x.size(0)\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN BOTH MODELS\n",
    "# ============================================================\n",
    "ae, vae = Autoencoder().to(device), VAE(latent_dim=config[\"latent_dim\"]).to(device)\n",
    "opt_ae = optim.Adam(ae.parameters(), lr=config[\"learning_rate\"])\n",
    "opt_vae = optim.Adam(vae.parameters(), lr=config[\"learning_rate\"])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "ae_train, ae_val, vae_train, vae_val = [], [], [], []\n",
    "\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    # ---- AE ----\n",
    "    ae.train(); total_loss = 0\n",
    "    for imgs,_ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        recon = ae(imgs)\n",
    "        loss = criterion(recon, imgs)\n",
    "        opt_ae.zero_grad(); loss.backward(); opt_ae.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    ae.train_loss = total_loss / len(train_loader.dataset)\n",
    "    ae_val_loss = 0\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs,_ in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            loss = criterion(ae(imgs), imgs)\n",
    "            ae_val_loss += loss.item() * imgs.size(0)\n",
    "    ae_val_loss /= len(val_loader.dataset)\n",
    "    ae_train.append(ae.train_loss); ae_val.append(ae_val_loss)\n",
    "\n",
    "    # ---- VAE ----\n",
    "    vae.train(); total_loss = 0\n",
    "    for imgs,_ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        recon, mu, logvar = vae(imgs)\n",
    "        loss = vae_loss(recon, imgs, mu, logvar)\n",
    "        opt_vae.zero_grad(); loss.backward(); opt_vae.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "    vae.train_loss = total_loss / len(train_loader.dataset)\n",
    "    vae_val_loss = 0\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs,_ in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            recon, mu, logvar = vae(imgs)\n",
    "            loss = vae_loss(recon, imgs, mu, logvar)\n",
    "            vae_val_loss += loss.item() * imgs.size(0)\n",
    "    vae_val_loss /= len(val_loader.dataset)\n",
    "    vae_train.append(vae.train_loss); vae_val.append(vae_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']} | AE: {ae.train_loss:.4f}/{ae_val_loss:.4f} | VAE: {vae.train_loss:.4f}/{vae_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LEARNING CURVES\n",
    "# ============================================================\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(ae_train, label=\"AE Train\"); plt.plot(ae_val, label=\"AE Val\")\n",
    "plt.plot(vae_train, label=\"VAE Train\"); plt.plot(vae_val, label=\"VAE Val\")\n",
    "plt.xlabel(\"Epochs\"); plt.ylabel(\"Loss\"); plt.title(\"AE vs VAE Learning Curves\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92712c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RECONSTRUCTIONS\n",
    "# ============================================================\n",
    "model_list = [(\"Autoencoder\", ae), (\"Variational Autoencoder\", vae)]\n",
    "\n",
    "for name, model in model_list:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        imgs,_ = next(iter(test_loader))\n",
    "        imgs = imgs[:8].to(device)\n",
    "        if isinstance(model, VAE):\n",
    "            recon,_,_ = model(imgs)\n",
    "        else:\n",
    "            recon = model(imgs)\n",
    "        imgs, recon = imgs.cpu(), recon.cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2,8, figsize=(15,4))\n",
    "    for i in range(8):\n",
    "        axes[0,i].imshow(np.transpose(imgs[i], (1,2,0))); axes[0,i].axis(\"off\")\n",
    "        axes[1,i].imshow(np.transpose(recon[i], (1,2,0))); axes[1,i].axis(\"off\")\n",
    "    axes[0,0].set_ylabel(\"Original\"); axes[1,0].set_ylabel(name)\n",
    "    plt.suptitle(name + \" Reconstructions\")\n",
    "    #plt.savefig(name+\"VAEvsAEreconstruction.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f0f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RANDOM LATENT SAMPLES: AE vs VAE\n",
    "# ============================================================\n",
    "with torch.no_grad():\n",
    "    # --- VAE: sample from N(0, I) in latent space ---\n",
    "    z_vae = torch.randn(8, config[\"latent_dim\"]).to(device)\n",
    "    vae_samples = vae.decode(z_vae).cpu()\n",
    "\n",
    "    # --- AE: random latent tensor (same shape as encoder output) ---\n",
    "    z_ae = torch.randn(8, 128, config[\"image_size\"] // 8, config[\"image_size\"] // 8).to(device)\n",
    "    ae_samples = ae.decoder(z_ae).cpu()\n",
    "\n",
    "# ---- Plot both ----\n",
    "fig, axes = plt.subplots(2, 8, figsize=(15, 4))\n",
    "\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(np.transpose(ae_samples[i], (1, 2, 0)))\n",
    "    axes[0, i].axis(\"off\")\n",
    "    axes[1, i].imshow(np.transpose(vae_samples[i], (1, 2, 0)))\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"AE\", fontsize=12)\n",
    "axes[1, 0].set_ylabel(\"VAE\", fontsize=12)\n",
    "#plt.suptitle(\"Random Latent Samples: AE vs VAE\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"VAEvsAEreconstructionRandomz.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Times\"],\n",
    "    \"mathtext.fontset\": \"cm\",\n",
    "    \"axes.unicode_minus\": False,\n",
    "    \"text.latex.preamble\": r\"\\usepackage{amsmath}\",  # <-- add this\n",
    "})\n",
    "# ============================================================\n",
    "# SPARSE MEASUREMENT RECONSTRUCTION (AE vs VAE)\n",
    "# ============================================================\n",
    "def sparse_reconstruction(model, is_vae=False, n_samples=5000, n_iters=800, lr=1e-2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_imgs,_ = next(iter(test_loader))\n",
    "    test_img = test_imgs[0:1].to(device)\n",
    "    img_size = config[\"image_size\"]\n",
    "\n",
    "    # --- create 2D sampling mask ---\n",
    "    mask2d = torch.zeros((1,1,img_size,img_size), device=device)\n",
    "    xy_idx = torch.randint(0, img_size*img_size, (n_samples,), device=device)\n",
    "    mask2d.view(-1)[xy_idx] = 1.0\n",
    "    mask = mask2d.repeat(1,3,1,1)\n",
    "    measured = test_img * mask\n",
    "\n",
    "    # --- latent initialization ----\n",
    "    if is_vae:\n",
    "        latent_dim = config[\"latent_dim\"]\n",
    "        z = torch.randn(1, latent_dim, device=device, requires_grad=True)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn_like(test_img)\n",
    "            z = model.encoder(dummy)\n",
    "        z = torch.randn_like(z).to(device).requires_grad_(True)\n",
    "\n",
    "    optimizer_z = optim.Adam([z], lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # --- optimize latent code ---\n",
    "    for i in range(n_iters):\n",
    "        optimizer_z.zero_grad()\n",
    "        if is_vae:\n",
    "            recon = model.decode(z)\n",
    "        else:\n",
    "            recon = model.decoder(z)\n",
    "        loss = loss_fn(recon * mask, measured)\n",
    "        loss.backward()\n",
    "        optimizer_z.step()\n",
    "        if (i+1)%200==0 or i==0:\n",
    "            print(f\"{'VAE' if is_vae else 'AE'} Iter {i+1}/{n_iters} | Loss {loss.item():.6e}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if is_vae:\n",
    "            recon_img = model.decode(z).cpu().squeeze(0)\n",
    "        else:\n",
    "            recon_img = model.decoder(z).cpu().squeeze(0)\n",
    "        sparse_obs = measured.cpu().squeeze(0)\n",
    "        orig = test_img.cpu().squeeze(0)\n",
    "    return orig, sparse_obs, recon_img\n",
    "\n",
    "# ---- Run sparse reconstruction for AE and VAE ----\n",
    "orig_ae, obs_ae, recon_ae = sparse_reconstruction(ae, is_vae=False, n_samples=200)\n",
    "orig_vae, obs_vae, recon_vae = sparse_reconstruction(vae, is_vae=True, n_samples=200)\n",
    "\n",
    "# ---- Visualization ----\n",
    "def to_img(t): return np.transpose(t.numpy(), (1,2,0))\n",
    "fig, axes = plt.subplots(2, 3, figsize=(10,5))\n",
    "axes[0,0].imshow(to_img(orig_ae));  axes[0,0].set_title(\"Original\");  axes[0,0].axis(\"off\")\n",
    "axes[0,1].imshow(to_img(obs_ae));   axes[0,1].set_title(\"Observed (AE)\"); axes[0,1].axis(\"off\")\n",
    "axes[0,2].imshow(to_img(recon_ae)); axes[0,2].set_title(\"AE Reconstruction\"); axes[0,2].axis(\"off\")\n",
    "\n",
    "axes[1,0].imshow(to_img(orig_vae));  axes[1,0].set_title(\"Original\");  axes[1,0].axis(\"off\")\n",
    "axes[1,1].imshow(to_img(obs_vae));   axes[1,1].set_title(\"Observed (VAE)\"); axes[1,1].axis(\"off\")\n",
    "axes[1,2].imshow(to_img(recon_vae)); axes[1,2].set_title(\"VAE Reconstruction\"); axes[1,2].axis(\"off\")\n",
    "\n",
    "#plt.suptitle(\"Sparse Measurement Reconstruction (n=2000 pixels)\")\n",
    "#plt.savefig(\"VAEvsAEreconstruction200.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ham",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
